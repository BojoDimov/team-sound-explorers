# Music style transfer
&nbsp;&nbsp;&nbsp;&nbsp;
Крайната цел на проекта е генеративен модел, чрез който по две зададени песни да се генерира трета която притежава определени сходни
характеристики с изходните песни. Например: новата песен може да има същото темпо/ритъм като първата изходяща песен, но текстурата (timbre) да бъде извлечена от втората песен. 

&nbsp;&nbsp;&nbsp;&nbsp;
Проектът ще бъде с изследователски характер, като ще изпробваме различни подходи и модели на базата на вече създадени технологии. Изследванията ще бъдат инкрементални, тоест ще започнем с извличане на ноти от един инструмент и пренасянето им към друг инструмент. Постепенно можем да усложняваме проекта, като за момента някои от крайните идеите са: пренасяне от цял жанр към друг, забавяне/забързване на темпото на песента, промяна на вокалите на песен спрямо вокалите на друга песен.

## Подход/Алгоритъм
&nbsp;&nbsp;&nbsp;&nbsp;
Нашите модели ще бъдат модификации на вече съществуващи модели на WaveNet и Nsynth които се базират на dillated conv layers. Ще се опитаме да приложим последните проучвания по темата като например capsule nets. Това са модели за които контекстът на извадката е важен и за тази цел се използват техники и похвати за разширяване на контекста.

За дейтасетове имаме две неща наум:
1. [NSynth dataset](https://magenta.tensorflow.org/datasets/nsynth) съдържащ 306к музикални ноти към над 1000 инструмента
2. Колекция от музикални изпълнения (за момента на пиано) от които ще се опитваме да извличаме разнообразни embeddings

### База/Вече направен research/Помощни ресурси
1. [WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) използван най-вече за text-to-speech, [официален пейпър](https://arxiv.org/pdf/1609.03499.pdf)
2. [NSynth](https://magenta.tensorflow.org/nsynth-fastgen)
3. [Dillated Convolutions](http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation) много добра статия обясняваща предимствата на такъв тип конволюционни слоеве
4. [Audio texture synthesis](https://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/) един от първите успешни модели (малко остарял вече рисърч, все пак моде да бъде използван за общ поглед към темата)
5. [How Shazam Works](http://coding-geek.com/how-shazam-works/) много добра статия за задълбочаване на това как се представят аудио сигналите и различните им характеристики като pitch, timbre, envelope и т.н
6. [Librosa](http://librosa.github.io/librosa/advanced.html) фреймуърк на пайтън за трансфорамция на аудио. Имат множество примери и визуализации на важни характеристики на музиката.
7. [Notes on music information retrieval](https://musicinformationretrieval.com/index.html) много полезен ресурс свързан със изследване на музикални произведения, който също включва и реализация на някои прости ML модели и също няколко модела за music decomposition.
8. [Имплементация на Nsynth](https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth)

### Оценка на резултата
На по-нисък етап ще гледаме колко добре нотите в изходната песен съвпадат с тези в новополучената песен.
За момента, за по-комплексните задачи се сещаме за два начина на оценка:
1. Чрез слушане на произведената песен
2. Чрез сравняване на извлечени данни за 3-те песни. Например можем да сравняваме една от изходните ни песни и резултата на базата на `tempo estimation`, и заедно с това да сравняваме другата изходна песен и резултата на базата на `Mel-Frequency Cepstral Coefficients`
