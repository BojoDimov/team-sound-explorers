{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\deepLearning\\team-sound-explorers\\wavenet_test\\masked.py:116: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _condition(x, encoding):\n",
    "    \"\"\"Condition the input on the encoding.\n",
    "    Args:\n",
    "      x: The [mb, length, channels] float tensor input.\n",
    "      encoding: The [mb, encoding_length, channels] float tensor encoding.\n",
    "    Returns:\n",
    "      The output after broadcasting the encoding to x's shape and adding them.\n",
    "    \"\"\"\n",
    "    mb, length, channels = x.get_shape().as_list()\n",
    "    enc_mb, enc_length, enc_channels = encoding.get_shape().as_list()\n",
    "    assert enc_mb == mb\n",
    "    assert enc_channels == channels\n",
    "\n",
    "    encoding = tf.reshape(encoding, [mb, enc_length, 1, channels])\n",
    "    x = tf.reshape(x, [mb, enc_length, -1, channels])\n",
    "    x += encoding\n",
    "    x = tf.reshape(x, [mb, length, channels])\n",
    "    x.set_shape([mb, length, channels])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self, inputs, is_training):\n",
    "    \"\"\"Build the graph for this configuration.\n",
    "    Args:\n",
    "      inputs: A dict of inputs. For training, should contain 'wav'.\n",
    "      is_training: Whether we are training or not. Not used in this config.\n",
    "    Returns:\n",
    "      A dict of outputs that includes the 'predictions', 'loss', the 'encoding',\n",
    "      the 'quantized_input', and whatever metrics we want to track for eval.\n",
    "    \"\"\"\n",
    "    del is_training\n",
    "    num_stages = 10\n",
    "    num_layers = 30\n",
    "    filter_length = 3\n",
    "    width = 512\n",
    "    skip_width = 256\n",
    "    ae_num_stages = 10\n",
    "    ae_num_layers = 30\n",
    "    ae_filter_length = 3\n",
    "    ae_width = 128\n",
    "    \n",
    "    ae_hop_length = 512\n",
    "    ae_bottleneck_width = 16\n",
    "\n",
    "    # Encode the source with 8-bit Mu-Law.\n",
    "    x = inputs['wav']\n",
    "    x_quantized = utils.mu_law(x)\n",
    "    x_scaled = tf.cast(x_quantized, tf.float32) / 128.0\n",
    "    x_scaled = tf.expand_dims(x_scaled, 2)\n",
    "\n",
    "    ###\n",
    "    # The Non-Causal Temporal Encoder.\n",
    "    ###\n",
    "    en = masked.conv1d(\n",
    "        x_scaled,\n",
    "        causal=False,\n",
    "        num_filters=ae_width,\n",
    "        filter_length=ae_filter_length,\n",
    "        name='ae_startconv')\n",
    "\n",
    "    for num_layer in range(ae_num_layers):\n",
    "      dilation = 2**(num_layer % ae_num_stages)\n",
    "      d = tf.nn.relu(en)\n",
    "      d = masked.conv1d(\n",
    "          d,\n",
    "          causal=False,\n",
    "          num_filters=ae_width,\n",
    "          filter_length=ae_filter_length,\n",
    "          dilation=dilation,\n",
    "          name='ae_dilatedconv_%d' % (num_layer + 1))\n",
    "      d = tf.nn.relu(d)\n",
    "      en += masked.conv1d(\n",
    "          d,\n",
    "          num_filters=ae_width,\n",
    "          filter_length=1,\n",
    "          name='ae_res_%d' % (num_layer + 1))\n",
    "\n",
    "    en = masked.conv1d(\n",
    "        en,\n",
    "        num_filters=self.ae_bottleneck_width,\n",
    "        filter_length=1,\n",
    "        name='ae_bottleneck')\n",
    "    en = masked.pool1d(en, self.ae_hop_length, name='ae_pool', mode='avg')\n",
    "    encoding = en\n",
    "\n",
    "    ###\n",
    "    # The WaveNet Decoder.\n",
    "    ###\n",
    "    l = masked.shift_right(x_scaled)\n",
    "    l = masked.conv1d(\n",
    "        l, num_filters=width, filter_length=filter_length, name='startconv')\n",
    "\n",
    "    # Set up skip connections.\n",
    "    s = masked.conv1d(\n",
    "        l, num_filters=skip_width, filter_length=1, name='skip_start')\n",
    "\n",
    "    # Residual blocks with skip connections.\n",
    "    for i in range(num_layers):\n",
    "      dilation = 2**(i % num_stages)\n",
    "      d = masked.conv1d(\n",
    "          l,\n",
    "          num_filters=2 * width,\n",
    "          filter_length=filter_length,\n",
    "          dilation=dilation,\n",
    "          name='dilatedconv_%d' % (i + 1))\n",
    "      d = self._condition(d,\n",
    "                          masked.conv1d(\n",
    "                              en,\n",
    "                              num_filters=2 * width,\n",
    "                              filter_length=1,\n",
    "                              name='cond_map_%d' % (i + 1)))\n",
    "\n",
    "      assert d.get_shape().as_list()[2] % 2 == 0\n",
    "      m = d.get_shape().as_list()[2] // 2\n",
    "      d_sigmoid = tf.sigmoid(d[:, :, :m])\n",
    "      d_tanh = tf.tanh(d[:, :, m:])\n",
    "      d = d_sigmoid * d_tanh\n",
    "\n",
    "      l += masked.conv1d(\n",
    "          d, num_filters=width, filter_length=1, name='res_%d' % (i + 1))\n",
    "      s += masked.conv1d(\n",
    "          d, num_filters=skip_width, filter_length=1, name='skip_%d' % (i + 1))\n",
    "\n",
    "    s = tf.nn.relu(s)\n",
    "    s = masked.conv1d(s, num_filters=skip_width, filter_length=1, name='out1')\n",
    "    s = self._condition(s,\n",
    "                        masked.conv1d(\n",
    "                            en,\n",
    "                            num_filters=skip_width,\n",
    "                            filter_length=1,\n",
    "                            name='cond_map_out1'))\n",
    "    s = tf.nn.relu(s)\n",
    "\n",
    "    ###\n",
    "    # Compute the logits and get the loss.\n",
    "    ###\n",
    "    logits = masked.conv1d(s, num_filters=256, filter_length=1, name='logits')\n",
    "    logits = tf.reshape(logits, [-1, 256])\n",
    "    probs = tf.nn.softmax(logits, name='softmax')\n",
    "    x_indices = tf.cast(tf.reshape(x_quantized, [-1]), tf.int32) + 128\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=x_indices, name='nll'),\n",
    "        0,\n",
    "        name='loss')\n",
    "\n",
    "    return {\n",
    "        'predictions': probs,\n",
    "        'loss': loss,\n",
    "        'eval': {\n",
    "            'nll': loss\n",
    "        },\n",
    "        'quantized_input': x_quantized,\n",
    "        'encoding': encoding,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 200000\n",
    "learning_rate_schedule = {\n",
    "    0: 2e-4,\n",
    "    90000: 4e-4 / 3,\n",
    "    120000: 6e-5,\n",
    "    150000: 4e-5,\n",
    "    180000: 2e-5,\n",
    "    210000: 6e-6,\n",
    "    240000: 2e-6,\n",
    "}\n",
    "\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-4042c7f2f799>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-4042c7f2f799>\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    slim.learning.train(\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    total_batch_size = 128\n",
    "\n",
    "    # Run the Reader on the CPU\n",
    "\n",
    "#       inputs_dict = \n",
    "\n",
    "    global_step = tf.get_variable(\n",
    "        \"global_step\", \n",
    "        [],\n",
    "        tf.int32,\n",
    "        initializer=tf.constant_initializer(0),\n",
    "        trainable=False)\n",
    "    \n",
    "    outputs_dict = build(inputs_dict, is_training=True)\n",
    "    loss = outputs_dict['loss']\n",
    "  \n",
    "    tf.summary.scalar(\"train_loss\", loss)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(\n",
    "        decay=0.9999, num_updates=global_step)\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(lr, epsilon=1e-8)\n",
    "\n",
    "    train_op = opt.minimize(\n",
    "        loss,\n",
    "        global_step=global_step,\n",
    "        name=\"train\",\n",
    "        colocate_gradients_with_ops=True\n",
    "        \n",
    "    \n",
    "    slim.learning.train(\n",
    "        train_op=train_op,\n",
    "        logdir=logdir,\n",
    "        is_chief=is_chief,\n",
    "        master=FLAGS.master,\n",
    "        number_of_steps=config.num_iters,\n",
    "        global_step=global_step,\n",
    "        log_every_n_steps=250,\n",
    "        local_init_op=local_init_op,\n",
    "        save_interval_secs=300,\n",
    "        sync_optimizer=opt,\n",
    "        session_config=session_config,)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
