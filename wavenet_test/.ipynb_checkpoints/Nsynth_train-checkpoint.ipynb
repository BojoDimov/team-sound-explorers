{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\deepLearning\\team-sound-explorers\\wavenet_test\\masked.py:116: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _condition(x, encoding):\n",
    "    \"\"\"Condition the input on the encoding.\n",
    "    Args:\n",
    "      x: The [mb, length, channels] float tensor input.\n",
    "      encoding: The [mb, encoding_length, channels] float tensor encoding.\n",
    "    Returns:\n",
    "      The output after broadcasting the encoding to x's shape and adding them.\n",
    "    \"\"\"\n",
    "    mb, length, channels = x.get_shape().as_list()\n",
    "    enc_mb, enc_length, enc_channels = encoding.get_shape().as_list()\n",
    "    assert enc_mb == mb\n",
    "    assert enc_channels == channels\n",
    "\n",
    "    encoding = tf.reshape(encoding, [mb, enc_length, 1, channels])\n",
    "    x = tf.reshape(x, [mb, enc_length, -1, channels])\n",
    "    x += encoding\n",
    "    x = tf.reshape(x, [mb, length, channels])\n",
    "    x.set_shape([mb, length, channels])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self, inputs, is_training):\n",
    "    \"\"\"Build the graph for this configuration.\n",
    "    Args:\n",
    "      inputs: A dict of inputs. For training, should contain 'wav'.\n",
    "      is_training: Whether we are training or not. Not used in this config.\n",
    "    Returns:\n",
    "      A dict of outputs that includes the 'predictions', 'loss', the 'encoding',\n",
    "      the 'quantized_input', and whatever metrics we want to track for eval.\n",
    "    \"\"\"\n",
    "    del is_training\n",
    "    num_stages = 10\n",
    "    num_layers = 30\n",
    "    filter_length = 3\n",
    "    width = 512\n",
    "    skip_width = 256\n",
    "    ae_num_stages = 10\n",
    "    ae_num_layers = 30\n",
    "    ae_filter_length = 3\n",
    "    ae_width = 128\n",
    "    \n",
    "    ae_hop_length = 512\n",
    "    ae_bottleneck_width = 16\n",
    "\n",
    "    # Encode the source with 8-bit Mu-Law.\n",
    "    x = inputs['wav']\n",
    "    x_quantized = utils.mu_law(x)\n",
    "    x_scaled = tf.cast(x_quantized, tf.float32) / 128.0\n",
    "    x_scaled = tf.expand_dims(x_scaled, 2)\n",
    "\n",
    "    ###\n",
    "    # The Non-Causal Temporal Encoder.\n",
    "    ###\n",
    "    en = masked.conv1d(\n",
    "        x_scaled,\n",
    "        causal=False,\n",
    "        num_filters=ae_width,\n",
    "        filter_length=ae_filter_length,\n",
    "        name='ae_startconv')\n",
    "\n",
    "    for num_layer in range(ae_num_layers):\n",
    "      dilation = 2**(num_layer % ae_num_stages)\n",
    "      d = tf.nn.relu(en)\n",
    "      d = masked.conv1d(\n",
    "          d,\n",
    "          causal=False,\n",
    "          num_filters=ae_width,\n",
    "          filter_length=ae_filter_length,\n",
    "          dilation=dilation,\n",
    "          name='ae_dilatedconv_%d' % (num_layer + 1))\n",
    "      d = tf.nn.relu(d)\n",
    "      en += masked.conv1d(\n",
    "          d,\n",
    "          num_filters=ae_width,\n",
    "          filter_length=1,\n",
    "          name='ae_res_%d' % (num_layer + 1))\n",
    "\n",
    "    en = masked.conv1d(\n",
    "        en,\n",
    "        num_filters=self.ae_bottleneck_width,\n",
    "        filter_length=1,\n",
    "        name='ae_bottleneck')\n",
    "    en = masked.pool1d(en, self.ae_hop_length, name='ae_pool', mode='avg')\n",
    "    encoding = en\n",
    "\n",
    "    ###\n",
    "    # The WaveNet Decoder.\n",
    "    ###\n",
    "    l = masked.shift_right(x_scaled)\n",
    "    l = masked.conv1d(\n",
    "        l, num_filters=width, filter_length=filter_length, name='startconv')\n",
    "\n",
    "    # Set up skip connections.\n",
    "    s = masked.conv1d(\n",
    "        l, num_filters=skip_width, filter_length=1, name='skip_start')\n",
    "\n",
    "    # Residual blocks with skip connections.\n",
    "    for i in range(num_layers):\n",
    "      dilation = 2**(i % num_stages)\n",
    "      d = masked.conv1d(\n",
    "          l,\n",
    "          num_filters=2 * width,\n",
    "          filter_length=filter_length,\n",
    "          dilation=dilation,\n",
    "          name='dilatedconv_%d' % (i + 1))\n",
    "      d = self._condition(d,\n",
    "                          masked.conv1d(\n",
    "                              en,\n",
    "                              num_filters=2 * width,\n",
    "                              filter_length=1,\n",
    "                              name='cond_map_%d' % (i + 1)))\n",
    "\n",
    "      assert d.get_shape().as_list()[2] % 2 == 0\n",
    "      m = d.get_shape().as_list()[2] // 2\n",
    "      d_sigmoid = tf.sigmoid(d[:, :, :m])\n",
    "      d_tanh = tf.tanh(d[:, :, m:])\n",
    "      d = d_sigmoid * d_tanh\n",
    "\n",
    "      l += masked.conv1d(\n",
    "          d, num_filters=width, filter_length=1, name='res_%d' % (i + 1))\n",
    "      s += masked.conv1d(\n",
    "          d, num_filters=skip_width, filter_length=1, name='skip_%d' % (i + 1))\n",
    "\n",
    "    s = tf.nn.relu(s)\n",
    "    s = masked.conv1d(s, num_filters=skip_width, filter_length=1, name='out1')\n",
    "    s = self._condition(s,\n",
    "                        masked.conv1d(\n",
    "                            en,\n",
    "                            num_filters=skip_width,\n",
    "                            filter_length=1,\n",
    "                            name='cond_map_out1'))\n",
    "    s = tf.nn.relu(s)\n",
    "\n",
    "    ###\n",
    "    # Compute the logits and get the loss.\n",
    "    ###\n",
    "    logits = masked.conv1d(s, num_filters=256, filter_length=1, name='logits')\n",
    "    logits = tf.reshape(logits, [-1, 256])\n",
    "    probs = tf.nn.softmax(logits, name='softmax')\n",
    "    x_indices = tf.cast(tf.reshape(x_quantized, [-1]), tf.int32) + 128\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=x_indices, name='nll'),\n",
    "        0,\n",
    "        name='loss')\n",
    "\n",
    "    return {\n",
    "        'predictions': probs,\n",
    "        'loss': loss,\n",
    "        'eval': {\n",
    "            'nll': loss\n",
    "        },\n",
    "        'quantized_input': x_quantized,\n",
    "        'encoding': encoding,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 200000\n",
    "learning_rate_schedule = {\n",
    "    0: 2e-4,\n",
    "    90000: 4e-4 / 3,\n",
    "    120000: 6e-5,\n",
    "    150000: 4e-5,\n",
    "    180000: 2e-5,\n",
    "    210000: 6e-6,\n",
    "    240000: 2e-6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    total_batch_size = 128\n",
    "\n",
    "    # Run the Reader on the CPU\n",
    "\n",
    "#       inputs_dict = \n",
    "\n",
    "      global_step = tf.get_variable(\n",
    "          \"global_step\", [],\n",
    "          tf.int32,\n",
    "          initializer=tf.constant_initializer(0),\n",
    "          trainable=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
